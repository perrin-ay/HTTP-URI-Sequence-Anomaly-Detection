{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "wFhXnhPNuYCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset,Value, Sequence, Features\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding, AutoModelForMaskedLM\n",
        "from transformers import default_data_collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from transformers import AutoConfig, AutoModelForMaskedLM, BertForMaskedLM\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "import time,sys,os\n",
        "import math\n",
        "import collections\n",
        "import itertools\n",
        "import random\n"
      ],
      "metadata": {
        "id": "2bMbyZxLuNUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load trained (domain adapted to HTTP REST API requests to Alteon ) distilbert model**\n",
        "\n",
        "- This trained model can be found on huggingface\n",
        "\n",
        "https://huggingface.co/bridge4\n"
      ],
      "metadata": {
        "id": "EhAbB8cxugBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelhiddenextract = DistilBertModel.from_pretrained(\"4epochdistilbert_uri_domainadapt\")\n",
        "print (modelhiddenextract)"
      ],
      "metadata": {
        "id": "-WWAX5OSuCA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature extraction using trained distilbert model**\n",
        "\n",
        "- Extract embeddings of each HTTP request URI by passing the URI through the trained model and then mean pooling the hidden states."
      ],
      "metadata": {
        "id": "Swb2h6PnDgRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelhiddenextract.to(device)\n",
        "\n",
        "def extract_meanpooled_hidden_states(batch):\n",
        "\n",
        "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #BaseModelOutput() with attributes:  last_hidden_state, hidden_states, attention\n",
        "        last_hidden_state = modelhiddenextract(inputs[\"input_ids\"],\n",
        "                                               attention_mask=inputs[\"attention_mask\"]).last_hidden_state\n",
        "\n",
        "    input_mask_expanded = (inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float())\n",
        "    meanpooled = torch.sum(last_hidden_state, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    return {\"hidden_state\": meanpooled.cpu().numpy()}\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "tokenized_datasets_hidden = tokenized_datasets.map(extract_meanpooled_hidden_states, batched=True, batch_size=100) # default batch_size=1000\n",
        "\n",
        "tokenized_datasets_hidden.save_to_disk(\"hiddenextractsavedds_meanpool\")\n",
        "\n",
        "dataset_hidden = tokenized_datasets_hidden.remove_columns([\"input_ids\",\"attention_mask\"])\n"
      ],
      "metadata": {
        "id": "z_dNknQmw182"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create legitimite and illegimate HTTP sequences from dataset**"
      ],
      "metadata": {
        "id": "Xvp_bOCAIPaf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6imPoidZrb7z"
      },
      "outputs": [],
      "source": [
        "dataset_hidden.set_format(type=\"pandas\")\n",
        "df = dataset_hidden[:]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## get indexes to create sequences ###################\n",
        "# Taking a HTTP request sequence to be randomize between 5-10 sequential requests\n",
        "\n",
        "indices = [i for i, s in enumerate(df[\"URI\"])]\n",
        "batches = []\n",
        "idxtmp = 0\n",
        "for i in range(0, len(indices)):\n",
        "\n",
        "    if i == 0:\n",
        "        idxtmp = random.randint(5,15)\n",
        "        batches.append(indices[i:i + idxtmp])\n",
        "        idxtmp = i + idxtmp #0+7\n",
        "    elif i <=idxtmp-1:\n",
        "        pass\n",
        "    else:\n",
        "        idxtmp = random.randint(5,15)\n",
        "        batches.append(indices[i:i + idxtmp])\n",
        "        idxtmp = i + idxtmp\n",
        "\n",
        "print (len(batches), batches[:5],batches[len(batches)-1], batches[len(batches)-2])"
      ],
      "metadata": {
        "id": "PO8MLditr2ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### using indexes from previous cell create sequences/patterns of HTTP requests and save as new HF dataset########\n",
        "############## labels 0 represent legitimate HTTP request sequences #################\n",
        "newuricol =[]\n",
        "newhidcol = []\n",
        "labells = []\n",
        "for batch in batches:\n",
        "    newuricol.append(df[\"URI\"].iloc[batch].values)\n",
        "    tmp = np.concatenate(df[\"hidden_state\"].iloc[batch].values)\n",
        "    tmp=tmp.reshape([len(batch),768])\n",
        "    newhidcol.append(tmp)\n",
        "\n",
        "labells = [0]*len(newuricol)\n",
        "\n",
        "newds = Dataset.from_dict({\"URI\":newuricol,\"hidden_state\":newhidcol,\"label\":labells})\n"
      ],
      "metadata": {
        "id": "AeBmYS0Fr590"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### new dataframe used for classification task ##############\n",
        "dfclass = pd.DataFrame(columns=[\"URI\",\"hidden_state\"])\n",
        "dfclass[\"URI\"]=newuricol\n",
        "dfclass[\"hidden_state\"]=newhidcol\n",
        "dfclass[\"label\"] = [0]*len(dfclass[\"URI\"]) # 0 label for good sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "YmxnCH4ytTBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This dataset will be used for two different downstream modeling tasks**\n",
        "\n",
        "- **Classification with transformer encoder**\n",
        "\n",
        "For this create a new dataset where we permute the HTTP requests within a sequence and assign a new label 1 for anomolous\n",
        "\n",
        "- **Unsupervised learning using LSTM VAE**\n",
        "\n",
        "Here we continue to use the original dataset of sequences , labels not required."
      ],
      "metadata": {
        "id": "OHRY14lDKZwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## For classification task create anomolous sequences by permuting within sequence #############\n",
        "\n",
        "dfanom = df.sample(frac=0.75, random_state=72).reset_index(drop=True)\n",
        "indices = [i for i, s in enumerate(dfanom[\"URI\"])]\n",
        "batches = []\n",
        "idxtmp = 0\n",
        "for i in range(0, len(indices)):\n",
        "\n",
        "    if i == 0:\n",
        "        idxtmp = random.randint(5,15)\n",
        "        batches.append(indices[i:i + idxtmp])\n",
        "        idxtmp = i + idxtmp #0+7\n",
        "\n",
        "    elif i <=idxtmp-1:\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "        idxtmp = random.randint(5,15) # idx =7; 6\n",
        "\n",
        "        batches.append(indices[i:i + idxtmp])\n",
        "        idxtmp = i + idxtmp #7+7+6\n",
        "\n",
        "newuricol =[]\n",
        "newhidcol = []\n",
        "labells = []\n",
        "\n",
        "for batch in batches:\n",
        "    newuricol.append(dfanom[\"URI\"].iloc[batch].values)\n",
        "    tmp = np.concatenate(dfanom[\"hidden_state\"].iloc[batch].values)\n",
        "    tmp=tmp.reshape([len(batch),768])\n",
        "    newhidcol.append(tmp)\n",
        "labells = [1]*len(newuricol)\n",
        "\n",
        "newdsanom = Dataset.from_dict({\"URI\":newuricol,\"hidden_state\":newhidcol,\"label\":labells})"
      ],
      "metadata": {
        "id": "7jeXB3cgtZwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### concat legitimate and anomolous datasets for training in classification task ####################\n",
        "\n",
        "ds_concat = datasets.concatenate_datasets([newds, newdsanom])\n",
        "ds_concat = ds_concat.shuffle(seed=42)\n",
        "ds_concat.save_to_disk(\"ds_concat_classif_meanpooled\")"
      ],
      "metadata": {
        "id": "-wDS0wC9tjjJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}